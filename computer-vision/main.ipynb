{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6f7757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/Data-Science-Projects/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms as T\n",
    "from pytorch_metric_learning import losses, samplers\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from timm.data import resolve_data_config\n",
    "import timm\n",
    "from transformers import AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2808c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    \"\"\"Get best available device\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2837ef",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25487212",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaguarTrainDataset(Dataset):\n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.img_dir = Path(img_dir)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Create label mapping\n",
    "        unique_jaguars = sorted(self.df['ground_truth'].unique())\n",
    "        self.label_map = {name: idx for idx, name in enumerate(unique_jaguars)}\n",
    "        self.num_classes = len(unique_jaguars)\n",
    "\n",
    "        print(f\"Found {self.num_classes} unique jaguars\")\n",
    "        print(f\"Total training images: {len(self.df)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = self.img_dir / row['filename']\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.label_map[row['ground_truth']]\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class JaguarTestDataset(Dataset):\n",
    "    \"\"\"Dataset for extracting embeddings from test images\"\"\"\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = Path(test_dir)\n",
    "        # Get all test images\n",
    "        self.image_files = sorted(self.test_dir.glob('*.png'))\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"Found {len(self.image_files)} test images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, img_path.name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a11f3b",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85bf4bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_stats(model_name='dinov2_vitb14'):\n",
    "    \n",
    "    # Map model names to Hugging Face identifiers\n",
    "    model_map = {\n",
    "        'dinov2_vits14': 'facebook/dinov2-small',\n",
    "        'dinov2_vitb14': 'facebook/dinov2-base',\n",
    "        'dinov2_vitl14': 'facebook/dinov2-large',\n",
    "    }\n",
    "    \n",
    "    hf_name = model_map.get(model_name, 'facebook/dinov2-base')\n",
    "    \n",
    "    print(f\"Loading normalization from {hf_name}...\")\n",
    "    processor = AutoImageProcessor.from_pretrained(hf_name)\n",
    "    \n",
    "    mean = processor.image_mean\n",
    "    std = processor.image_std\n",
    "    \n",
    "    print(f\"  mean: {mean}\")\n",
    "    print(f\"  std: {std}\")\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'std': std\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da403107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(img_size=224, model_name='dinov2_vitb14'):\n",
    "    \"\"\"\n",
    "    Create train and test transforms with model-specific normalization.\n",
    "    \n",
    "    Args:\n",
    "        img_size: Target image size (default: 224)\n",
    "        model_name: Name of the pretrained model to match normalization\n",
    "    \n",
    "    Returns:\n",
    "        train_transform, test_transform\n",
    "    \"\"\"\n",
    "    # Load normalization stats from model\n",
    "    norm_stats = get_normalization_stats(model_name)\n",
    "    \n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.RandomHorizontalFlip(p=0.5),\n",
    "        T.RandomRotation(degrees=15),\n",
    "        T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.1),\n",
    "        T.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=norm_stats['mean'], std=norm_stats['std'])\n",
    "    ])\n",
    "    \n",
    "    test_transform = T.Compose([\n",
    "        T.Resize((img_size, img_size)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=norm_stats['mean'], std=norm_stats['std'])\n",
    "    ])\n",
    "    \n",
    "    return train_transform, test_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926c61c9",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0605dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaguarReIDModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, backbone='dinov2'):\n",
    "        super().__init__()\n",
    "\n",
    "        if backbone == 'dinov2':\n",
    "            # DINOv2 ViT-Base\n",
    "            print(\"Loading DINOv2 ViT-Base...\")\n",
    "            self.backbone = torch.hub.load('facebookresearch/dinov2',\n",
    "                                          'dinov2_vitb14')\n",
    "            backbone_dim = 768\n",
    "        elif backbone == 'dinov2_small':\n",
    "            print(\"Loading DINOv2 ViT-Small (faster for CPU/MPS)...\")\n",
    "            self.backbone = torch.hub.load('facebookresearch/dinov2',\n",
    "                                          'dinov2_vits14')\n",
    "            backbone_dim = 384\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
    "\n",
    "        # Projection head to desired embedding dimension\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(backbone_dim, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embeddings = self.projection(features)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ace44",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5fc0867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_csv, train_dir, num_epochs=20, batch_size=32,\n",
    "                embedding_dim=512, lr=1e-4, device='cpu', img_size=224, backbone='dinov2_small'):\n",
    "\n",
    "    # Setup transforms\n",
    "    train_transform, _ = get_transforms(img_size=img_size)\n",
    "\n",
    "    # Create dataset\n",
    "    train_dataset = JaguarTrainDataset(train_csv, train_dir, train_transform)\n",
    "    num_classes = train_dataset.num_classes\n",
    "\n",
    "    # Create labels array for sampler\n",
    "    labels = [train_dataset.label_map[row['ground_truth']]\n",
    "              for _, row in train_dataset.df.iterrows()]\n",
    "\n",
    "    # Balanced sampler (4 images per jaguar per batch)\n",
    "    sampler = samplers.MPerClassSampler(\n",
    "        labels=labels,\n",
    "        m=4,  # 4 images per class\n",
    "        length_before_new_iter=len(train_dataset)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=16,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Create model\n",
    "    model = JaguarReIDModel(embedding_dim=embedding_dim,\n",
    "                           backbone=backbone).to(device)\n",
    "\n",
    "    # ArcFace loss\n",
    "    loss_func = losses.ArcFaceLoss(\n",
    "        num_classes=num_classes,\n",
    "        embedding_size=embedding_dim,\n",
    "        margin=28.6,  # degrees\n",
    "        scale=64\n",
    "    ).to(device)\n",
    "\n",
    "    # Optimizer for both model and loss function\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters(), 'lr': lr},\n",
    "        {'params': loss_func.parameters(), 'lr': lr}\n",
    "    ])\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_epochs\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    print(f\"\\nStarting training for {num_epochs} epochs...\")\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        for images, labels_batch in pbar:\n",
    "            images = images.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            embeddings = model(images)\n",
    "            loss = loss_func(embeddings, labels_batch)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Avg Loss: {avg_loss:.4f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc16a10",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98d9dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_test_embeddings(model, test_dir, device='cpu'):\n",
    "    \"\"\"Extract embeddings for all test images\"\"\"\n",
    "    _, test_transform = get_transforms()\n",
    "\n",
    "    test_dataset = JaguarTestDataset(test_dir, test_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=16,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    print(\"\\nExtracting test embeddings...\")\n",
    "    with torch.no_grad():\n",
    "        for images, filenames in tqdm(test_loader):\n",
    "            images = images.to(device)\n",
    "            embeddings = model(images)\n",
    "\n",
    "            # Normalize embeddings for cosine similarity\n",
    "            embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "\n",
    "            for emb, fname in zip(embeddings, filenames):\n",
    "                embeddings_dict[fname] = emb.cpu().numpy()\n",
    "\n",
    "    return embeddings_dict\n",
    "\n",
    "\n",
    "def create_submission(embeddings_dict, test_csv, output_path):\n",
    "    \"\"\"Create submission file from embeddings\"\"\"\n",
    "    test_df = pd.read_csv(test_csv)\n",
    "\n",
    "    print(\"\\nComputing similarities...\")\n",
    "    similarities = []\n",
    "\n",
    "    for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        query_emb = embeddings_dict[row['query_image']]\n",
    "        gallery_emb = embeddings_dict[row['gallery_image']]\n",
    "\n",
    "        # Cosine similarity (already normalized, so just dot product)\n",
    "        sim = np.dot(query_emb, gallery_emb)\n",
    "\n",
    "        # Map from [-1, 1] to [0, 1]\n",
    "        sim = (sim + 1) / 2\n",
    "\n",
    "        # Clip to ensure valid range\n",
    "        sim = np.clip(sim, 0.0, 1.0)\n",
    "\n",
    "        similarities.append(sim)\n",
    "\n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'similarity': similarities\n",
    "    })\n",
    "\n",
    "    # Validate\n",
    "    assert len(submission) == 137270, f\"Wrong number of rows: {len(submission)}\"\n",
    "    assert (submission['similarity'] >= 0).all(), \"Found negative values\"\n",
    "    assert (submission['similarity'] <= 1).all(), \"Found values > 1\"\n",
    "\n",
    "    # Save\n",
    "    submission.to_csv(output_path, index=False)\n",
    "\n",
    "    print(f\"\\n✓ Submission saved to {output_path}\")\n",
    "    print(f\"  Rows: {len(submission):,}\")\n",
    "    print(f\"  Similarity range: [{submission['similarity'].min():.4f}, {submission['similarity'].max():.4f}]\")\n",
    "    print(f\"  Similarity mean: {submission['similarity'].mean():.4f}\")\n",
    "\n",
    "    return submission\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f159b2",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6865b315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 4090\n",
      "GPU Memory: 25.25 GB\n",
      "Loading normalization from facebook/dinov2-base...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  mean: [0.485, 0.456, 0.406]\n",
      "  std: [0.229, 0.224, 0.225]\n",
      "Found 31 unique jaguars\n",
      "Total training images: 1895\n",
      "Loading DINOv2 ViT-Base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for 200 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200: 100%|██████████| 30/30 [00:37<00:00,  1.26s/it, loss=36.4938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Avg Loss: 34.7338, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200: 100%|██████████| 30/30 [00:36<00:00,  1.22s/it, loss=34.9135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Avg Loss: 33.0621, LR: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200:  53%|█████▎    | 16/30 [00:21<00:04,  3.32it/s, loss=31.7813]"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Paths\n",
    "    TRAIN_CSV = 'jaguar-re-id/train.csv'\n",
    "    TRAIN_DIR = 'jaguar-re-id/train/train'\n",
    "    TEST_CSV = 'jaguar-re-id/test.csv'\n",
    "    TEST_DIR = 'jaguar-re-id/test/test'\n",
    "    OUTPUT_CSV = 'submission.csv'\n",
    "    MODEL_PATH = 'jaguar_reid_model.pth'\n",
    "    \n",
    "    # Hyperparameters (adjusted for MacOS)\n",
    "    NUM_EPOCHS = 200\n",
    "    BATCH_SIZE = 64\n",
    "    EMBEDDING_DIM = 512\n",
    "    LEARNING_RATE = 1e-4\n",
    "    IMG_SIZE = 224\n",
    "    BACKBONE = 'dinov2'\n",
    "\n",
    "    # Get device\n",
    "    DEVICE = get_device()\n",
    "\n",
    "    # Train model\n",
    "    model = train_model(\n",
    "        train_csv=TRAIN_CSV,\n",
    "        train_dir=TRAIN_DIR,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        lr=LEARNING_RATE,\n",
    "        device=DEVICE,\n",
    "        img_size=IMG_SIZE,\n",
    "        backbone=BACKBONE\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    print(f\"\\n✓ Model saved to {MODEL_PATH}\")\n",
    "    \n",
    "    # Extract test embeddings\n",
    "    embeddings_dict = extract_test_embeddings(model, TEST_DIR, device=DEVICE)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = create_submission(embeddings_dict, TEST_CSV, OUTPUT_CSV)\n",
    "    \n",
    "    print(\"\\n✓ Done!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983aee97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
